--- cloop.h
+++ cloop.h
@@ -1,3 +1,7 @@
+#define CLOOP_SIGNATURE "#!/bin/sh"                      /* @ offset 0  */
+#define CLOOP_SIGNATURE_SIZE 9
+#define CLOOP_SIGNATURE_OFFSET 0x0
+
 #ifndef _COMPRESSED_LOOP_H
 #define _COMPRESSED_LOOP_H
 
@@ -38,10 +42,6 @@
 
 #include <linux/types.h>   /* u_int32_t */
 
-#ifndef __KERNEL__
-#include <stdint.h> /* regular uint64_t */
-#endif
-
 #define CLOOP_HEADROOM 128
 
 /* Header of fixed length, can be located at beginning or end of file   */
@@ -52,13 +52,6 @@
 	u_int32_t num_blocks;
 };
 
-#define CLOOP2_SIGNATURE "V2.0"                       /* @ offset 0x0b  */
-#define CLOOP2_SIGNATURE_SIZE 4
-#define CLOOP2_SIGNATURE_OFFSET 0x0b
-#define CLOOP4_SIGNATURE "V4.0"                       /* @ offset 0x0b  */
-#define CLOOP4_SIGNATURE_SIZE 4
-#define CLOOP4_SIGNATURE_OFFSET 0x0b
-
 /************************************************************************\
 *  CLOOP4 flags for each compressed block                                *
 *  Value   Meaning                                                       *
@@ -84,6 +77,134 @@
 
 #define CLOOP_COMPRESSOR_VALID(x) ((x) >= CLOOP_COMPRESSOR_ZLIB && (x) <= CLOOP_COMPRESSOR_LZO1X)
 
+#define CLOOP_COMPRESSOR_LINK  0xF
+
+
+/* data_index (num_blocks 64bit pointers, network order)...      */
+/* compressed data (gzip block compressed format)...             */
+
+struct cloop_tail
+{
+	u_int32_t table_size; 
+	u_int32_t index_size; /* size:4 unused:3 ctrl-c:1 lastlen:24 */
+#define CLOOP3_INDEX_SIZE(x)    ((unsigned int)((x) & 0xF))
+	u_int32_t num_blocks;
+};
+
+#define GZIP_MAX_BUFFER(n)	((n) + (n)/1000 + 12)
+
+struct block_info
+{
+	loff_t offset;		/* 64-bit offsets of compressed block */
+	u_int32_t size;		/* 32-bit compressed block size */
+	u_int32_t flags;	/* 32-bit compression flags */
+};
+
+static inline char *build_index(struct block_info *offsets, unsigned long n, 
+			unsigned long block_size)
+{
+	static char v[11];
+	u_int32_t flags = 0;
+	u_int32_t *ofs32 = (u_int32_t *) offsets;
+	loff_t    *ofs64 = (loff_t *) offsets;
+
+	/* v3 64bits bug: v1 assumed */
+	unsigned long	v3_64 = (n+1)/2;
+	loff_t	prev;
+
+	if (ofs32[0] != 0 && ofs32[1] == 0) {
+		for (prev=__le64_to_cpu(ofs64[v3_64]);
+		     v3_64 > 0 && __le64_to_cpu(ofs64[--v3_64]) < prev;
+		     prev=__le64_to_cpu(ofs64[v3_64]));
+	}
+
+	if (ofs32[0] == 0) {
+		if (ofs32[2]) { /* ACCELERATED KNOPPIX V1.0 */
+			while (n--) {
+				offsets[n].offset = __be64_to_cpu(offsets[n].offset);
+				offsets[n].size = ntohl(offsets[n].size);
+				offsets[n].flags = 0;
+			}
+			return (char *) "128BE accelerated knoppix 1.0";
+		}
+		else { /* V2.0/V4.0 */
+			loff_t last = CLOOP_BLOCK_OFFSET(__be64_to_cpu(ofs64[n]));
+			unsigned long i = n;
+
+			for (flags = 0; n-- ;) {
+				loff_t data = __be64_to_cpu(ofs64[n]); 
+
+				offsets[n].size = last - 
+					(offsets[n].offset = CLOOP_BLOCK_OFFSET(data)); 
+				last = offsets[n].offset;
+				offsets[n].flags = CLOOP_BLOCK_FLAGS(data); 
+				flags |= 1 << offsets[n].flags;
+			}
+			if (flags < 2) return (char *) "64BE v2.0";
+			while (i--) {
+				if (offsets[i].flags == CLOOP_COMPRESSOR_LINK) {
+					offsets[i] = offsets[offsets[i].offset];
+				}
+			}
+			strcpy(v, (char *) "64BE v4.0a");
+		}
+	}
+	else if (ofs32[1] == 0 && v3_64 == 0) { /* V1.0 */
+		loff_t last = __le64_to_cpu(ofs64[n]);
+		while (n--) {
+			offsets[n].size = last - 
+				(offsets[n].offset = __le64_to_cpu(ofs64[n])); 
+			last = offsets[n].offset;
+			offsets[n].flags = 0;
+		}
+		return (char *) "64LE v1.0";
+	}
+	else { /* V3.0 or V0.68 */
+		unsigned long i;
+		loff_t j;
+		
+		for (i = 0; i < n && ntohl(ofs32[i]) < ntohl(ofs32[i+1]); i++);
+		if (i == n && ntohl(ofs32[0]) == (4*n) + 0x8C) { /* V0.68 */
+			loff_t last = ntohl(ofs32[n]);
+			while (n--) {
+				offsets[n].size = last - 
+					(offsets[n].offset = ntohl(ofs32[n])); 
+				last = offsets[n].offset;
+				offsets[n].flags = 0;
+			}
+			return (char *) "32BE v0.68";
+		}
+		
+		v3_64 = (ofs32[1] == 0);
+		for (i = n; i-- != 0; ) {
+			offsets[i].size = ntohl(ofs32[i << v3_64]); 
+			if (offsets[i].size == 0xFFFFFFFF) {
+				offsets[i].size = 0x10000000 | block_size;
+			}
+			offsets[i].flags = (offsets[i].size >> 28);
+			offsets[i].size &= 0x0FFFFFFF; 
+		}
+		for (i = 0, j = sizeof(struct cloop_head); i < n; i++) {
+			offsets[i].offset = j;
+			if (offsets[i].flags < 8) {
+				j += offsets[i].size;
+			}
+		}
+		for (i = 0; i < n; i++) {
+			flags |= 1 << offsets[i].flags;
+			if (offsets[i].flags >= 8) {
+				offsets[i] = offsets[offsets[i].size];
+			}
+		}
+		strcpy(v, (char *) (v3_64) ? "64BE v3.0a" : "32BE v3.0a");
+	}
+	v[10] = 'a' + ((flags-1) & 0xF);	// compressors used
+	if (flags > 0x10) {			// with links ?
+		v[10] += 'A' - 'a';
+	}
+	return v;
+}
+
 /* Cloop suspend IOCTL */
 #define CLOOP_SUSPEND 0x4C07
 
--- cloop.c
+++ cloop.c
@@ -17,7 +17,7 @@
 \************************************************************************/
 
 #define CLOOP_NAME "cloop"
-#define CLOOP_VERSION "5.3"
+#define CLOOP_VERSION "4.12"
 #define CLOOP_MAX 8
 
 #ifndef KBUILD_MODNAME
@@ -68,7 +68,6 @@
 #include <linux/loop.h>
 #include <linux/kthread.h>
 #include <linux/compat.h>
-#include <linux/blk-mq.h> /* new multiqueue infrastructure */
 #include "cloop.h"
 
 /* New License scheme */
@@ -93,10 +92,7 @@
 /* Use experimental major for now */
 #define MAJOR_NR 240
 
-#ifndef DEVICE_NAME
-#define DEVICE_NAME CLOOP_NAME
-#endif
-
+/* #define DEVICE_NAME CLOOP_NAME */
 /* #define DEVICE_NR(device) (MINOR(device)) */
 /* #define DEVICE_ON(device) */
 /* #define DEVICE_OFF(device) */
@@ -143,7 +139,7 @@
  u_int32_t allflags;
 
  /* An array of cloop_ptr flags/offset for compressed blocks within the file */
- cloop_block_ptr *block_ptrs;
+ struct block_info *block_ptrs;
 
  /* We buffer some uncompressed blocks for performance */
  size_t num_buffered_blocks;	/* how many uncompressed blocks buffered for performance */
@@ -178,14 +174,16 @@
  spinlock_t queue_lock;
  /* mutex for ioctl() */
  struct mutex clo_ctl_mutex;
- /* mutex for request */
- struct mutex clo_rq_mutex;
+ struct list_head clo_list;
+ struct task_struct *clo_thread;
+ wait_queue_head_t clo_event;
  struct request_queue *clo_queue;
  struct gendisk *clo_disk;
- struct blk_mq_tag_set tag_set;
  int suspended;
 };
 
+/* Changed in 2.639: cloop_dev is now a an array of cloop_dev pointers,
+   so we can specify how many devices we need via parameters. */
 static struct cloop_device **cloop_dev;
 static const char *cloop_name=CLOOP_NAME;
 static int cloop_count = 0;
@@ -214,24 +212,21 @@
  vfree(mem);
 }
 
-/* static int uncompress(struct cloop_device *clo, unsigned char *dest, unsigned long *destLen, unsigned char *source, unsigned long sourceLen) */
-static int uncompress(struct cloop_device *clo, u_int32_t block_num, u_int32_t compressed_length, unsigned long *uncompressed_length)
+static int uncompress(struct cloop_device *clo, unsigned char *dest, unsigned long *destLen, unsigned char *source, unsigned long sourceLen, int flags) 
 {
  int err = -1;
- int flags = CLOOP_BLOCK_FLAGS(clo->block_ptrs[block_num]);
  switch(flags)
  {
   case CLOOP_COMPRESSOR_NONE:
-   /* block is umcompressed, swap pointers only! */
-   { char *tmp = clo->compressed_buffer; clo->compressed_buffer = clo->buffer[clo->current_bufnum]; clo->buffer[clo->current_bufnum] = tmp; }
-   DEBUGP("cloop: block %d is uncompressed (flags=%d), just swapping %u bytes\n", block_num, flags, compressed_length);
+   memcpy(dest, source, *destLen = sourceLen);
+   err = Z_OK;
    break;
 #if (defined(CONFIG_ZLIB_INFLATE) || defined(CONFIG_ZLIB_INFLATE_MODULE))
   case CLOOP_COMPRESSOR_ZLIB:
-   clo->zstream.next_in = clo->compressed_buffer;
-   clo->zstream.avail_in = compressed_length;
-   clo->zstream.next_out = clo->buffer[clo->current_bufnum];
-   clo->zstream.avail_out = clo->head.block_size;
+   clo->zstream.next_in = source;
+   clo->zstream.avail_in = sourceLen;
+   clo->zstream.next_out = dest;
+   clo->zstream.avail_out = *destLen;
    err = zlib_inflateReset(&clo->zstream);
    if (err != Z_OK)
    {
@@ -239,50 +234,50 @@
     zlib_inflateEnd(&clo->zstream); zlib_inflateInit(&clo->zstream);
    }
    err = zlib_inflate(&clo->zstream, Z_FINISH);
-   *uncompressed_length = clo->zstream.total_out;
+   *destLen = clo->zstream.total_out;
    if (err == Z_STREAM_END) err = 0;
-   DEBUGP("cloop: zlib decompression done, ret =%d, size =%lu\n", err, *uncompressed_length);
+   DEBUGP("cloop: zlib decompression done, ret =%d, size =%lu\n", err, *destLen);
    break;
 #endif
 #if (defined(CONFIG_LZO_DECOMPRESS) || defined(CONFIG_LZO_DECOMPRESS_MODULE))
   case CLOOP_COMPRESSOR_LZO1X:
    {
     size_t tmp = (size_t) clo->head.block_size;
-    err = lzo1x_decompress_safe(clo->compressed_buffer, compressed_length,
-             clo->buffer[clo->current_bufnum], &tmp);
-    if (err == LZO_E_OK) *uncompressed_length = (u_int32_t) tmp;
+    err = lzo1x_decompress_safe(source, sourceLen,
+             dest, &tmp);
+    if (err == LZO_E_OK) *destLen = (u_int32_t) tmp;
    }
    break;
 #endif
 #if (defined(CONFIG_DECOMPRESS_LZ4) || defined(CONFIG_DECOMPRESS_LZ4_MODULE))
   case CLOOP_COMPRESSOR_LZ4:
    {
-    size_t outputSize = clo->head.block_size;
+    size_t outputSize = *destLen;
     /* We should adjust outputSize here, in case the last block is smaller than block_size */
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 11, 0) /* field removed */
-    err = lz4_decompress(clo->compressed_buffer, (size_t *) &compressed_length,
-                         clo->buffer[clo->current_bufnum], outputSize);
+    err = lz4_decompress(source, (size_t *) &sourceLen,
+                         dest, outputSize);
 #else
-    err = LZ4_decompress_safe(clo->compressed_buffer,
-                              clo->buffer[clo->current_bufnum],
-                              compressed_length, outputSize);
+    err = LZ4_decompress_safe(source,
+                              dest,
+                              sourceLen, outputSize);
 #endif
     if (err >= 0) 
     {
      err = 0;
-     *uncompressed_length = outputSize;
+     *destLen = outputSize;
     }
    }
   break;
 #endif
 #if (defined(CONFIG_DECOMPRESS_XZ) || defined(CONFIG_DECOMPRESS_XZ_MODULE))
  case CLOOP_COMPRESSOR_XZ:
-  clo->xz_buffer.in = clo->compressed_buffer;
+  clo->xz_buffer.in = source;
   clo->xz_buffer.in_pos = 0;
-  clo->xz_buffer.in_size = compressed_length;
-  clo->xz_buffer.out = clo->buffer[clo->current_bufnum];
+  clo->xz_buffer.in_size = sourceLen;
+  clo->xz_buffer.out = dest;
   clo->xz_buffer.out_pos = 0;
-  clo->xz_buffer.out_size = clo->head.block_size;
+  clo->xz_buffer.out_size = *destLen;
   xz_dec_reset(clo->xzdecoderstate);
   err = xz_dec_run(clo->xzdecoderstate, &clo->xz_buffer);
   if (err == XZ_STREAM_END || err == XZ_OK)
@@ -309,16 +304,12 @@
  while (buf_done < buf_len)
   {
    size_t size = buf_len - buf_done, size_read;
-   mm_segment_t old_fs;
    /* kernel_read() only supports 32 bit offsets, so we use vfs_read() instead. */
    /* int size_read = kernel_read(f, pos, buf + buf_done, size); */
-
-   // mutex_lock(&clo->clo_rq_mutex);
-   old_fs = get_fs();
-   set_fs(KERNEL_DS);
+   mm_segment_t old_fs = get_fs();
+   set_fs(get_ds());
    size_read = vfs_read(f, (void __user *)(buf + buf_done), size, &pos);
    set_fs(old_fs);
-   // mutex_unlock(&clo->clo_rq_mutex);
 
    if(size_read <= 0)
     {
@@ -358,8 +349,8 @@
    return i;
   }
 
- compressed_block_offset = CLOOP_BLOCK_OFFSET(clo->block_ptrs[blocknum]);
- compressed_block_len = (long) (CLOOP_BLOCK_OFFSET(clo->block_ptrs[blocknum+1]) - compressed_block_offset) ;
+ compressed_block_offset = clo->block_ptrs[blocknum].offset;
+ compressed_block_len = (long) (clo->block_ptrs[blocknum].size) ;
 
  /* Load one compressed block from the file. */
  if(compressed_block_offset > 0 && compressed_block_len >= 0) /* sanity check */
@@ -369,12 +360,12 @@
   if (n!= compressed_block_len)
    {
     printk(KERN_ERR "%s: error while reading %lu bytes @ %llu from file %s\n",
-     cloop_name, compressed_block_len, clo->block_ptrs[blocknum], clo->underlying_filename);
+     cloop_name, compressed_block_len, clo->block_ptrs[blocknum].offset, clo->underlying_filename);
     /* return -1; */
    }
  } else {
   printk(KERN_ERR "%s: invalid data block len %ld bytes @ %lld from file %s\n",
-  cloop_name, compressed_block_len, clo->block_ptrs[blocknum], clo->underlying_filename);
+  cloop_name, compressed_block_len, clo->block_ptrs[blocknum].offset, clo->underlying_filename);
   return -1;
  }
   
@@ -382,14 +373,16 @@
  if(++clo->current_bufnum >= clo->num_buffered_blocks) clo->current_bufnum = 0;
 
  /* Do the uncompression */
- ret = uncompress(clo, blocknum, compressed_block_len, &uncompressed_block_len);
+ uncompressed_block_len = clo->head.block_size;
+ ret = uncompress(clo, clo->buffer[clo->current_bufnum], &uncompressed_block_len,
+	 clo->compressed_buffer, compressed_block_len, clo->block_ptrs[blocknum].flags);
  /* DEBUGP("cloop: buflen after uncompress: %ld\n",buflen); */
  if (ret != 0)
  {
   printk(KERN_ERR "%s: decompression error %i uncompressing block %u %lu bytes @ %llu, flags %u\n",
          cloop_name, ret, blocknum,
-         compressed_block_len, CLOOP_BLOCK_OFFSET(clo->block_ptrs[blocknum]),
-         CLOOP_BLOCK_FLAGS(clo->block_ptrs[blocknum]));
+         compressed_block_len, clo->block_ptrs[blocknum].offset,
+         clo->block_ptrs[blocknum].flags);
          clo->buffered_blocknum[clo->current_bufnum] = -1;
   return -1;
  }
@@ -397,107 +390,146 @@
  return clo->current_bufnum;
 }
 
-static blk_status_t cloop_handle_request(struct cloop_device *clo, struct request *req)
+/* This function does all the real work. */
+/* returns "uptodate"                    */
+static int cloop_handle_request(struct cloop_device *clo, struct request *req)
 {
  int buffered_blocknum = -1;
  int preloaded = 0;
- loff_t offset = (loff_t) blk_rq_pos(req)<<9;
+ loff_t offset     = (loff_t) blk_rq_pos(req)<<9; /* req->sector<<9 */
  struct bio_vec bvec;
  struct req_iterator iter;
- blk_status_t ret = BLK_STS_OK;
-
- if (unlikely(req_op(req) != REQ_OP_READ ))
- {
-  blk_dump_rq_flags(req, DEVICE_NAME " bad request");
-  return BLK_STS_IOERR;
- }
-
- if (unlikely(!clo->backing_file && !clo->suspended))
- {
-  DEBUGP("cloop_handle_request: not connected to a file\n");
-  return BLK_STS_IOERR;
- }
-
  rq_for_each_segment(bvec, req, iter)
- {
-  unsigned long len = bvec.bv_len;
-  loff_t to_offset  = bvec.bv_offset;
-
-  while(len > 0)
   {
-   u_int32_t length_in_buffer;
-   loff_t block_offset = offset;
-   u_int32_t offset_in_buffer;
-   char *from_ptr, *to_ptr;
-   /* do_div (div64.h) returns the 64bit division remainder and  */
-   /* puts the result in the first argument, i.e. block_offset   */
-   /* becomes the blocknumber to load, and offset_in_buffer the  */
-   /* position in the buffer */
-   offset_in_buffer = do_div(block_offset, clo->head.block_size);
-   /* Lookup preload cache */
-   if(block_offset < clo->preload_size && clo->preload_cache != NULL && clo->preload_cache[block_offset] != NULL)
-   { /* Copy from cache */
-    preloaded = 1;
-    from_ptr = clo->preload_cache[block_offset];
-   }
-   else
-   {
-    preloaded = 0;
-    buffered_blocknum = cloop_load_buffer(clo,block_offset);
-    if(buffered_blocknum == -1)
+   unsigned long len = bvec.bv_len;
+   char *to_ptr      = kmap(bvec.bv_page) + bvec.bv_offset;
+   while(len > 0)
     {
-     ret = BLK_STS_IOERR;
-     break; /* invalid data, leave inner loop */
+     u_int32_t length_in_buffer;
+     loff_t block_offset = offset;
+     u_int32_t offset_in_buffer;
+     char *from_ptr;
+     /* do_div (div64.h) returns the 64bit division remainder and  */
+     /* puts the result in the first argument, i.e. block_offset   */
+     /* becomes the blocknumber to load, and offset_in_buffer the  */
+     /* position in the buffer */
+     offset_in_buffer = do_div(block_offset, clo->head.block_size);
+     /* Lookup preload cache */
+     if(block_offset < clo->preload_size && clo->preload_cache != NULL &&
+        clo->preload_cache[block_offset] != NULL)
+      { /* Copy from cache */
+       preloaded = 1;
+       from_ptr = clo->preload_cache[block_offset];
+      }
+     else
+      {
+       preloaded = 0;
+       buffered_blocknum = cloop_load_buffer(clo,block_offset);
+       if(buffered_blocknum == -1) break; /* invalid data, leave inner loop */
+       /* Copy from buffer */
+       from_ptr = clo->buffer[buffered_blocknum];
+      }
+     /* Now, at least part of what we want will be in the buffer. */
+     length_in_buffer = clo->head.block_size - offset_in_buffer;
+     if(length_in_buffer > len)
+      {
+/*   DEBUGP("Warning: length_in_buffer=%u > len=%u\n",
+                      length_in_buffer,len); */
+       length_in_buffer = len;
+      }
+     memcpy(to_ptr, from_ptr + offset_in_buffer, length_in_buffer);
+     to_ptr      += length_in_buffer;
+     len         -= length_in_buffer;
+     offset      += length_in_buffer;
+    } /* while inner loop */
+   kunmap(bvec.bv_page);
+   cond_resched();
+  } /* end rq_for_each_segment*/
+ return ((buffered_blocknum!=-1) || preloaded);
+}
+
+/* Adopted from loop.c, a kernel thread to handle physical reads and
+   decompression. */
+static int cloop_thread(void *data)
+{
+ struct cloop_device *clo = data;
+ current->flags |= PF_NOFREEZE;
+ set_user_nice(current, 10);
+ while (!kthread_should_stop()||!list_empty(&clo->clo_list))
+  {
+   int err;
+   err = wait_event_interruptible(clo->clo_event, !list_empty(&clo->clo_list) || 
+                                  kthread_should_stop());
+   if(unlikely(err))
+    {
+     DEBUGP(KERN_ERR "cloop thread activated on error!? Continuing.\n");
+     continue;
     }
-    /* Copy from buffer */
-    from_ptr = clo->buffer[buffered_blocknum];
-   }
-   /* Now, at least part of what we want will be in the buffer. */
-   length_in_buffer = clo->head.block_size - offset_in_buffer;
-   if(length_in_buffer > len)
-   {
-   /* DEBUGP("Warning: length_in_buffer=%u > len=%u\n", length_in_buffer,len); */
-    length_in_buffer = len;
-   }
-   to_ptr      = kmap_atomic(bvec.bv_page);
-   memcpy(to_ptr + to_offset, from_ptr + offset_in_buffer, length_in_buffer);
-   kunmap_atomic(to_ptr);
-   to_offset   += length_in_buffer;
-   len         -= length_in_buffer;
-   offset      += length_in_buffer;
-  } /* while inner loop */
- } /* rq_for_each_segment */
- return ret;
-}
-
-static blk_status_t cloop_queue_rq(struct blk_mq_hw_ctx *hctx, const struct blk_mq_queue_data *bd)
-{
-//  struct request_queue *q  = hctx->queue;
-//  struct cloop_device *clo = q->queuedata;
- struct request *req = bd->rq;
- struct cloop_device *clo = req->rq_disk->private_data;
- blk_status_t ret         = BLK_STS_OK;
-
-#if 1 /* Does it work when loading libraries? */
- /* Since we have a buffered block list as well as data to read */
- /* from disk (slow), and are (probably) never called from an   */
- /* interrupt, we use a simple mutex lock right here to ensure  */
- /* consistency.                                                */
-  mutex_lock(&clo->clo_rq_mutex);
- #else
-  spin_lock_irq(&clo->queue_lock);
- #endif
- blk_mq_start_request(req);
- do {
-  ret = cloop_handle_request(clo, req);
- } while(blk_update_request(req, ret, blk_rq_cur_bytes(req)));
- blk_mq_end_request(req, ret);
- #if 1 /* See above */
-  mutex_unlock(&clo->clo_rq_mutex);
- #else
-  spin_unlock_irq(&clo->queue_lock);
- #endif
- return ret;
+   if(!list_empty(&clo->clo_list))
+    {
+     struct request *req;
+     unsigned long flags;
+     int uptodate;
+     spin_lock_irq(&clo->queue_lock);
+     req = list_entry(clo->clo_list.next, struct request, queuelist);
+     list_del_init(&req->queuelist);
+     spin_unlock_irq(&clo->queue_lock);
+     uptodate = cloop_handle_request(clo, req);
+     spin_lock_irqsave(&clo->queue_lock, flags);
+     __blk_end_request_all(req, uptodate ? 0 : -EIO);
+     spin_unlock_irqrestore(&clo->queue_lock, flags);
+    }
+  }
+ DEBUGP(KERN_ERR "cloop_thread exited.\n");
+ return 0;
+}
+
+/* This is called by the kernel block queue management every now and then,
+ * with successive read requests qeued and sorted in a (hopefully)
+ * "most efficient way". spin_lock_irq() is being held by the kernel. */
+static void cloop_do_request(struct request_queue *q)
+{
+ struct request *req;
+ while((req = blk_fetch_request(q)) != NULL)
+  {
+   struct cloop_device *clo;
+   int rw;
+ /* quick sanity checks */
+   /* blk_fs_request() was removed in 2.6.36 */
+   if (unlikely(req == NULL
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 11, 0) /* field removed */
+   || (req->cmd_type != REQ_TYPE_FS)
+#endif
+   ))
+    goto error_continue;
+   rw = rq_data_dir(req);
+   if (unlikely(rw != READ
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 8, 0)
+                && rw != READA
+#endif
+    ))
+    {
+     DEBUGP("cloop_do_request: bad command\n");
+     goto error_continue;
+    }
+   clo = req->rq_disk->private_data;
+   if (unlikely(!clo->backing_file && !clo->suspended))
+    {
+     DEBUGP("cloop_do_request: not connected to a file\n");
+     goto error_continue;
+    }
+   list_add_tail(&req->queuelist, &clo->clo_list); /* Add to working list for thread */
+   wake_up(&clo->clo_event);    /* Wake up cloop_thread */
+   continue; /* next request */
+  error_continue:
+   DEBUGP(KERN_ERR "cloop_do_request: Discarding request %p.\n", req);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
+   req->errors++;
+#else
+   req->error_count++;
+#endif
+   __blk_end_request_all(req, -EIO);
+  }
 }
 
 /* Read header, flags and offsets from already opened file */
@@ -508,7 +540,7 @@
  char *bbuf=NULL;
  unsigned int bbuf_size = 0;
  const unsigned int header_size = sizeof(struct cloop_head);
- unsigned int i, offsets_read=0, total_offsets=0;
+ unsigned int i, total_offsets=0;
  loff_t fs_read_position = 0, header_pos[2];
  int isblkdev, bytes_read, error = 0;
  if (clo->suspended) return error;
@@ -581,29 +613,19 @@
     goto error_release;
    }
    memcpy(&clo->head, bbuf, header_size);
-   if (strncmp(bbuf+CLOOP4_SIGNATURE_OFFSET, CLOOP4_SIGNATURE, CLOOP4_SIGNATURE_SIZE)==0)
+   if (strncmp(bbuf+CLOOP_SIGNATURE_OFFSET, CLOOP_SIGNATURE, CLOOP_SIGNATURE_SIZE)==0)
    {
-    clo->file_format=4;
+    clo->file_format++;
     clo->head.block_size=ntohl(clo->head.block_size);
     clo->head.num_blocks=ntohl(clo->head.num_blocks);
     clo->header_first =  (i==0) ? 1 : 0;
-    printk(KERN_INFO "%s: file %s version %d, %d blocks of %d bytes, header %s.\n", cloop_name, clo->underlying_filename, clo->file_format, clo->head.num_blocks, clo->head.block_size, (i==0)?"first":"last");
-    break;
-   }
-   else if (strncmp(bbuf+CLOOP2_SIGNATURE_OFFSET, CLOOP2_SIGNATURE, CLOOP2_SIGNATURE_SIZE)==0)
-   {
-    clo->file_format=2;
-    clo->head.block_size=ntohl(clo->head.block_size);
-    clo->head.num_blocks=ntohl(clo->head.num_blocks);
-    clo->header_first =  (i==0) ? 1 : 0;
-    printk(KERN_INFO "%s: file %s version %d, %d blocks of %d bytes, header %s.\n", cloop_name, clo->underlying_filename, clo->file_format, clo->head.num_blocks, clo->head.block_size, (i==0)?"first":"last");
+    printk(KERN_INFO "%s: file %s, %d blocks of %d bytes, header %s.\n", cloop_name, clo->underlying_filename, clo->head.num_blocks, clo->head.block_size, (i==0)?"first":"last");
     break;
    }
   }
  if (clo->file_format == 0)
   {
-   printk(KERN_ERR "%s: Cannot read old 32-bit (version 0.68) images, "
-                   "please use an older version of %s for this file.\n",
+   printk(KERN_ERR "%s: Cannot detect %s format.\n",
                    cloop_name, cloop_name);
        error=-EBADF; goto error_release;
   }
@@ -613,67 +635,133 @@
           cloop_name, clo->head.block_size);
    error=-EBADF; goto error_release;
   }
- total_offsets=clo->head.num_blocks+1;
- if (!isblkdev && (sizeof(struct cloop_head)+sizeof(loff_t)*
+ total_offsets=clo->head.num_blocks;
+ if (!isblkdev && (sizeof(struct cloop_head)+sizeof(struct block_info)*
                       total_offsets > inode->i_size))
   {
    printk(KERN_ERR "%s: file %s too small for %u blocks\n",
           cloop_name, clo->underlying_filename, clo->head.num_blocks);
    error=-EBADF; goto error_release;
   }
- clo->block_ptrs = cloop_malloc(sizeof(cloop_block_ptr) * total_offsets);
- if (!clo->block_ptrs)
+ /* Allocate Memory for decompressors */
+#if (defined(CONFIG_ZLIB_INFLATE) || defined(CONFIG_ZLIB_INFLATE_MODULE))
+ clo->zstream.workspace = cloop_malloc(zlib_inflate_workspacesize());
+ if(!clo->zstream.workspace)
   {
-   printk(KERN_ERR "%s: out of kernel mem for offsets\n", cloop_name);
+   printk(KERN_ERR "%s: out of mem for zlib working area %u\n",
+          cloop_name, zlib_inflate_workspacesize());
    error=-ENOMEM; goto error_release;
   }
- /* Read them offsets! */
- if(clo->header_first)
+ zlib_inflateInit(&clo->zstream);
+#endif
+#if (defined(CONFIG_DECOMPRESS_XZ) || defined(CONFIG_DECOMPRESS_XZ_MODULE))
+#if XZ_INTERNAL_CRC32
+  /* This must be called before any other xz_* function to initialize the CRC32 lookup table. */
+  xz_crc32_init(void);
+#endif
+  clo->xzdecoderstate = xz_dec_init(XZ_SINGLE, 0);
+#endif
+ if (total_offsets + 1 == 0) /* Version 3 */
   {
-   fs_read_position = sizeof(struct cloop_head);
+   struct cloop_tail tail;
+   if (isblkdev)
+    {
+    /* No end of file: can't find index */
+     printk(KERN_ERR "%s: no V3 support for block device\n", 
+            cloop_name);
+     error=-EBADF; goto error_release;
+    }
+   bytes_read = cloop_read_from_file(clo, file, (void *) &tail,
+			inode->i_size - sizeof(struct cloop_tail),
+			sizeof(struct cloop_tail));
+   if (bytes_read == sizeof(struct cloop_tail))
+    {
+     unsigned long len, zlen;
+     int ret;
+     void *zbuf;
+     clo->head.num_blocks = ntohl(tail.num_blocks);
+     total_offsets = clo->head.num_blocks;
+     clo->block_ptrs = cloop_malloc(sizeof(struct block_info) * total_offsets);
+     zlen = ntohl(tail.table_size);
+     zbuf = cloop_malloc(zlen);
+     if (!clo->block_ptrs || !zbuf)
+      {
+       printk(KERN_ERR "%s: out of kernel mem for index\n", cloop_name);
+       error=-ENOMEM; goto error_release;
+      }
+     bytes_read = cloop_read_from_file(clo, file, zbuf,
+			inode->i_size - zlen - sizeof(struct cloop_tail),
+			zlen);
+     if (bytes_read != zlen)
+      {
+       printk(KERN_ERR "%s: can't read index\n", cloop_name);
+       error=-EBADF; goto error_release;
+      }
+     len = CLOOP3_INDEX_SIZE(ntohl(tail.index_size)) * total_offsets;
+     ret = uncompress(clo, (void *) clo->block_ptrs, &len, zbuf, zlen, CLOOP_COMPRESSOR_ZLIB);
+     cloop_free(zbuf, zlen);
+     if (ret != 0)
+      {
+        printk(KERN_ERR "%s: decompression error %i uncompressing index\n",
+               cloop_name, ret);
+       error=-EBADF; goto error_release;
+      }
+    }
+   else
+    {
+     printk(KERN_ERR "%s: can't find index\n", cloop_name);
+     error=-ENOMEM; goto error_release;
+    }
   }
  else
   {
-   fs_read_position = clo->underlying_total_size - sizeof(struct cloop_head) - total_offsets * sizeof(loff_t);
-  }
- for(offsets_read=0;offsets_read<total_offsets;)
-  {
-   size_t bytes_readable;
-   unsigned int num_readable, offset = 0;
-   bytes_readable = MIN(bbuf_size, clo->underlying_total_size - fs_read_position);
-   if(bytes_readable <= 0) break; /* Done */
-   bytes_read = cloop_read_from_file(clo, file, bbuf, fs_read_position, bytes_readable);
-   if(bytes_read != bytes_readable)
+   unsigned int n, total_bytes;
+   clo->block_ptrs = cloop_malloc(sizeof(struct block_info) * total_offsets);
+   if (!clo->block_ptrs)
+    {
+     printk(KERN_ERR "%s: out of kernel mem for offsets\n", cloop_name);
+     error=-ENOMEM; goto error_release;
+    }
+   /* Read them offsets! */
+   if(clo->header_first)
     {
-     printk(KERN_ERR "%s: Bad file %s, read() %lu bytes @ %llu returned %d.\n",
-            cloop_name, clo->underlying_filename, (unsigned long)clo->underlying_blksize, fs_read_position, (int)bytes_read);
-     error=-EBADF;
-     goto error_release;
+     total_bytes = total_offsets * sizeof(struct block_info);
+     fs_read_position = sizeof(struct cloop_head);
     }
-   /* remember where to read the next blk from file */
-   fs_read_position += bytes_read;
-   /* calculate how many offsets can be taken from current bbuf */
-   num_readable = MIN(total_offsets - offsets_read,
-                      bytes_read / sizeof(loff_t));
-   DEBUGP(KERN_INFO "cloop: parsing %d offsets %d to %d\n", num_readable, offsets_read, offsets_read+num_readable-1);
-   for (i=0,offset=0; i<num_readable; i++)
+   else
     {
-     loff_t tmp = be64_to_cpu( *(loff_t*) (bbuf+offset) );
-     if (i%50==0) DEBUGP(KERN_INFO "cloop: offset %03d: %llu\n", offsets_read, tmp);
-     if(offsets_read > 0)
+     total_bytes = total_offsets * sizeof(loff_t);
+     fs_read_position = clo->underlying_total_size - sizeof(struct cloop_head) - total_bytes;
+    }
+   for(n=0;n<total_bytes;)
+    {
+     size_t bytes_readable;
+     bytes_readable = MIN(bbuf_size, clo->underlying_total_size - fs_read_position);
+     if(bytes_readable <= 0) break; /* Done */
+     bytes_read = cloop_read_from_file(clo, file, bbuf, fs_read_position, bytes_readable);
+     if(bytes_read != bytes_readable)
       {
-       loff_t d = CLOOP_BLOCK_OFFSET(tmp) - CLOOP_BLOCK_OFFSET(clo->block_ptrs[offsets_read-1]);
-       if(d > clo->largest_block) clo->largest_block = d;
+       printk(KERN_ERR "%s: Bad file %s, read() %lu bytes @ %llu returned %d.\n",
+              cloop_name, clo->underlying_filename, (unsigned long)clo->underlying_blksize, fs_read_position, (int)bytes_read);
+       error=-EBADF;
+       goto error_release;
       }
-     clo->block_ptrs[offsets_read++] = tmp;
-     offset += sizeof(loff_t);
+     memcpy(((char *)clo->block_ptrs) + n, bbuf, bytes_read);
+     /* remember where to read the next blk from file */
+     fs_read_position += bytes_read;
+     n += bytes_read;
     }
   }
-  printk(KERN_INFO "%s: %s: %u blocks, %u bytes/block, largest block is %lu bytes.\n",
-         cloop_name, clo->underlying_filename, clo->head.num_blocks,
-         clo->head.block_size, clo->largest_block);
  {
   int i;
+  char *version = build_index(clo->block_ptrs, clo->head.num_blocks, clo->head.block_size);
+  clo->largest_block = 0;
+  for (i = 0; i < clo->head.num_blocks; i++)
+    if (clo->block_ptrs[i].size > clo->largest_block)
+      clo->largest_block = clo->block_ptrs[i].size;
+  printk(KERN_INFO "%s: %s: %s: %u blocks, %u bytes/block, largest block is %lu bytes.\n",
+         cloop_name, clo->underlying_filename, version, clo->head.num_blocks,
+         clo->head.block_size, clo->largest_block);
   clo->num_buffered_blocks = (buffers > 0 && clo->head.block_size >= 512) ?
                               (buffers / clo->head.block_size) : 1;
   clo->buffered_blocknum = cloop_malloc(clo->num_buffered_blocks * sizeof (u_int32_t));
@@ -705,36 +793,14 @@
           cloop_name, clo->largest_block);
    error=-ENOMEM; goto error_release_free_buffer;
   }
- /* Allocate Memory for decompressors */
-#if (defined(CONFIG_ZLIB_INFLATE) || defined(CONFIG_ZLIB_INFLATE_MODULE))
- clo->zstream.workspace = cloop_malloc(zlib_inflate_workspacesize());
- if(!clo->zstream.workspace)
-  {
-   printk(KERN_ERR "%s: out of mem for zlib working area %u\n",
-          cloop_name, zlib_inflate_workspacesize());
-   error=-ENOMEM; goto error_release_free_all;
-  }
- zlib_inflateInit(&clo->zstream);
-#endif
-#if (defined(CONFIG_DECOMPRESS_XZ) || defined(CONFIG_DECOMPRESS_XZ_MODULE))
-#if XZ_INTERNAL_CRC32
-  /* This must be called before any other xz_* function to initialize the CRC32 lookup table. */
-  xz_crc32_init(void);
-#endif
-  clo->xzdecoderstate = xz_dec_init(XZ_SINGLE, 0);
-#endif
- if(CLOOP_BLOCK_OFFSET(clo->block_ptrs[clo->head.num_blocks]) > clo->underlying_total_size)
+ set_capacity(clo->clo_disk, (sector_t)(clo->head.num_blocks*(clo->head.block_size>>9)));
+ clo->clo_thread = kthread_create(cloop_thread, clo, "cloop%d", cloop_num);
+ if(IS_ERR(clo->clo_thread))
   {
-   printk(KERN_ERR "%s: final offset wrong (%llu > %llu)\n",
-          cloop_name,
-	  CLOOP_BLOCK_OFFSET(clo->block_ptrs[clo->head.num_blocks]),
-          clo->underlying_total_size);
-#if (defined(CONFIG_ZLIB_INFLATE) || defined(CONFIG_ZLIB_INFLATE_MODULE))
-   cloop_free(clo->zstream.workspace, zlib_inflate_workspacesize()); clo->zstream.workspace=NULL;
-#endif
+   error = PTR_ERR(clo->clo_thread);
+   clo->clo_thread=NULL;
    goto error_release_free_all;
   }
- set_capacity(clo->clo_disk, (sector_t)(clo->head.num_blocks*(clo->head.block_size>>9)));
  if(preload > 0)
   {
    clo->preload_array_size = ((preload<=clo->head.num_blocks)?preload:clo->head.num_blocks);
@@ -780,6 +846,7 @@
      clo->preload_array_size = clo->preload_size = 0;
     }
   }
+ wake_up_process(clo->clo_thread);
  /* Uncheck */
  return error;
 error_release_free_all:
@@ -794,9 +861,13 @@
  }
  if (clo->buffered_blocknum) { cloop_free(clo->buffered_blocknum, sizeof(int)*clo->num_buffered_blocks); clo->buffered_blocknum=NULL; }
 error_release_free:
- cloop_free(clo->block_ptrs, sizeof(cloop_block_ptr) * total_offsets);
+ cloop_free(clo->block_ptrs, sizeof(struct block_info) * total_offsets);
  clo->block_ptrs=NULL;
 error_release:
+#if (defined(CONFIG_ZLIB_INFLATE) || defined(CONFIG_ZLIB_INFLATE_MODULE))
+ zlib_inflateEnd(&clo->zstream);
+ if(clo->zstream.workspace) { cloop_free(clo->zstream.workspace, zlib_inflate_workspacesize()); clo->zstream.workspace = NULL; }
+#endif
  if(bbuf) cloop_free(bbuf, clo->underlying_blksize);
  if(clo->underlying_filename) { kfree(clo->underlying_filename); clo->underlying_filename=NULL; }
  clo->backing_file=NULL;
@@ -829,6 +900,7 @@
  if(clo->refcnt > 1)	/* we needed one fd for the ioctl */
    return -EBUSY;
  if(filp==NULL) return -EINVAL;
+ if(clo->clo_thread) { kthread_stop(clo->clo_thread); clo->clo_thread=NULL; }
  if(filp!=initial_file)
   fput(filp);
  else
@@ -839,7 +911,7 @@
  clo->backing_file  = NULL;
  clo->backing_inode = NULL;
  if(clo->underlying_filename) { kfree(clo->underlying_filename); clo->underlying_filename=NULL; }
- if(clo->block_ptrs) { cloop_free(clo->block_ptrs, clo->head.num_blocks+1); clo->block_ptrs = NULL; }
+ if(clo->block_ptrs) { cloop_free(clo->block_ptrs, clo->head.num_blocks); clo->block_ptrs = NULL; }
  if(clo->preload_cache)
  {
   int i;
@@ -1054,15 +1126,15 @@
   case LOOP_CLR_FD:       /* Change arg */ 
   case LOOP_GET_STATUS64: /* Change arg */ 
   case LOOP_SET_STATUS64: /* Change arg */ 
-    return cloop_ioctl(bdev, mode, cmd, (unsigned long) compat_ptr(arg));
+	arg = (unsigned long) compat_ptr(arg);
   case LOOP_SET_STATUS:   /* unchanged */
   case LOOP_GET_STATUS:   /* unchanged */
   case LOOP_SET_FD:       /* unchanged */
   case LOOP_CHANGE_FD:    /* unchanged */
-    return cloop_ioctl(bdev, mode, cmd, arg);
-  default:
-    return -ENOIOCTLCMD;
+	return cloop_ioctl(bdev, mode, cmd, arg);
+	break;
  }
+ return -ENOIOCTLCMD;
 }
 #endif
 
@@ -1093,7 +1165,7 @@
  cloop_dev[cloop_num]->refcnt-=1;
 }
 
-static const struct block_device_operations clo_fops =
+static struct block_device_operations clo_fops =
 {
         owner:		THIS_MODULE,
         open:           cloop_open,
@@ -1105,12 +1177,6 @@
 	/* locked_ioctl ceased to exist in 2.6.36 */
 };
 
-static const struct blk_mq_ops cloop_mq_ops = {
-	.queue_rq       = cloop_queue_rq,
-/*	.init_request	= cloop_init_request, */
-/*	.complete	= cloop_complete_rq, */
-};
-
 static int cloop_register_blkdev(int major_nr)
 {
  return register_blkdev(major_nr, cloop_name);
@@ -1124,37 +1190,33 @@
 
 static int cloop_alloc(int cloop_num)
 {
- struct cloop_device *clo = (struct cloop_device *) cloop_malloc(sizeof(struct cloop_device));
+ struct cloop_device *clo = (struct cloop_device *) cloop_malloc(sizeof(struct cloop_device));;
  if(clo == NULL) goto error_out;
  cloop_dev[cloop_num] = clo;
  memset(clo, 0, sizeof(struct cloop_device));
  clo->clo_number = cloop_num;
- clo->tag_set.ops = &cloop_mq_ops;
- clo->tag_set.nr_hw_queues = 1;
- clo->tag_set.queue_depth = 128;
- clo->tag_set.numa_node = NUMA_NO_NODE;
- clo->tag_set.cmd_size = 0; /* No extra data needed */
- /* BLK_MQ_F_BLOCKING is extremely important if we want to call blocking functions like vfs_read */
- clo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
- clo->tag_set.driver_data = clo;
- if(blk_mq_alloc_tag_set(&clo->tag_set)) goto error_out_free_clo;
- clo->clo_queue = blk_mq_init_queue(&clo->tag_set);
- if(IS_ERR(clo->clo_queue))
+ clo->clo_thread = NULL;
+ init_waitqueue_head(&clo->clo_event);
+ spin_lock_init(&clo->queue_lock);
+ mutex_init(&clo->clo_ctl_mutex);
+ INIT_LIST_HEAD(&clo->clo_list);
+ clo->clo_queue = blk_init_queue(cloop_do_request, &clo->queue_lock);
+ if(!clo->clo_queue)
   {
    printk(KERN_ERR "%s: Unable to alloc queue[%d]\n", cloop_name, cloop_num);
-   goto error_out_free_tags;
+   goto error_out;
   }
  clo->clo_queue->queuedata = clo;
- blk_queue_max_hw_sectors(clo->clo_queue, BLK_DEF_MAX_SECTORS);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
+ queue_flag_set_unlocked(QUEUE_FLAG_NONROT, clo->clo_queue);
+ queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, clo->clo_queue);
+#endif
  clo->clo_disk = alloc_disk(1);
  if(!clo->clo_disk)
   {
    printk(KERN_ERR "%s: Unable to alloc disk[%d]\n", cloop_name, cloop_num);
-   goto error_out_free_queue;
+   goto error_disk;
   }
- spin_lock_init(&clo->queue_lock);
- mutex_init(&clo->clo_ctl_mutex);
- mutex_init(&clo->clo_rq_mutex);
  clo->clo_disk->major = cloop_major;
  clo->clo_disk->first_minor = cloop_num;
  clo->clo_disk->fops = &clo_fops;
@@ -1163,12 +1225,8 @@
  sprintf(clo->clo_disk->disk_name, "%s%d", cloop_name, cloop_num);
  add_disk(clo->clo_disk);
  return 0;
-error_out_free_queue:
+error_disk:
  blk_cleanup_queue(clo->clo_queue);
-error_out_free_tags:
- blk_mq_free_tag_set(&clo->tag_set);
-error_out_free_clo:
- cloop_free(clo, sizeof(struct cloop_device));
 error_out:
  return -ENOMEM;
 }
@@ -1179,7 +1237,6 @@
  if(clo == NULL) return;
  del_gendisk(clo->clo_disk);
  blk_cleanup_queue(clo->clo_queue);
- blk_mq_free_tag_set(&clo->tag_set);
  put_disk(clo->clo_disk);
  cloop_free(clo, sizeof(struct cloop_device));
  cloop_dev[cloop_num] = NULL;
--- cloop_suspend.c
+++ cloop_suspend.c
@@ -14,6 +14,7 @@
 #include <fcntl.h>
 #include <unistd.h>
 #include <stdio.h>
+#include <stdint.h>
 
 /* We don't use the structure, so that define does not hurt */
 #define dev_t int
